{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning | Multi-Class Classification Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import sqlite3 as db\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to sqlite\n",
    "cnx = db.connect('../data/data.sqlite')\n",
    "\n",
    "# Loading datasets into pandas dataframes\n",
    "df_train = pd.read_sql_query(\"SELECT * FROM train\", cnx)\n",
    "df_unseen = pd.read_sql_query(\"SELECT * FROM test\", cnx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at top 5 rows of the train dataset\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the top 5 rows of the unseen dataset\n",
    "df_unseen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the info for the train dataset\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the info for the unseen dataset\n",
    "df_unseen.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unseen dataset already can a class column with all empty rows which we need to predict. Therefore we should drop this column, when running it through our classifier.\n",
    "\n",
    "We will now focus on the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying missing values in df_train\n",
    "missing_values = df_train.isnull().sum()\n",
    "\n",
    "# Percentage of missing values\n",
    "missing_percentage = (df_train.isnull().sum() / len(df_train)) * 100\n",
    "\n",
    "# Display both count and percentage of missing values\n",
    "missing_data = pd.DataFrame({\n",
    "    'Missing Values': missing_values,\n",
    "    'Percentage': missing_percentage\n",
    "})\n",
    "\n",
    "print(missing_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying missing values in df_unseen\n",
    "missing_values = df_unseen.isnull().sum()\n",
    "\n",
    "# Percentage of missing values\n",
    "missing_percentage = (df_unseen.isnull().sum() / len(df_unseen)) * 100\n",
    "\n",
    "# Display both count and percentage of missing values\n",
    "missing_data = pd.DataFrame({\n",
    "    'Missing Values': missing_values,\n",
    "    'Percentage': missing_percentage\n",
    "})\n",
    "\n",
    "print(missing_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate entries \n",
    "print(f\"Duplicate Rows: {df_train.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate features\n",
    "print(f\"Duplicate Features: {df_train.T.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking for objects | Non-numeric values and their count\n",
    "for i in df_train.select_dtypes(include=\"object\").columns:\n",
    "    print(df_train[i].value_counts())\n",
    "    print(\"***\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration of Numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the distribution \n",
    "\n",
    "numeric_cols = df_train.select_dtypes(include='number').columns\n",
    "\n",
    "# Plot histograms\n",
    "df_train[numeric_cols].hist(bins=15, figsize=(15, 10))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Box plot to identify outliers\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    plt.subplot(3, (len(numeric_cols) + 2) // 3, i + 1)\n",
    "    sns.boxplot(y=df_train[col])\n",
    "    plt.title(col)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = df_train.select_dtypes(include='object').columns\n",
    "\n",
    "# Plot count plots for categorical variables\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    plt.subplot(3, (len(categorical_cols) + 2) // 3, i + 1)\n",
    "    sns.countplot(y=df_train[col])\n",
    "    plt.title(col)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualizing missing value\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(df_train.isnull(), cbar=False, cmap='viridis')\n",
    "plt.title('Missing Values Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Checking Feature Corelation | Pearson Correlation\n",
    "df_train_numeric = df_train.drop(['Storage', 'Music', 'Guitar'], axis=1)\n",
    "\n",
    "plt.figure(figsize=(25,20))\n",
    "sns.heatmap(df_train_numeric.corr(), annot=True, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "We have **31 columns** with a lot of missing values and non numeric rows.\n",
    "\n",
    "* *Music*, *Storage*,  *Guitar* columns need to be converted into numeric form\n",
    "* Columns with missing values:\n",
    "  * *Tennis* = 50 rows. Can be guessed\n",
    "  * *Oven* = 1118 rows.  22.36% values missing. Can be guessed\n",
    "  * *Office* = 3008 rows. 60.16% values missing. Better to discard column. \n",
    "* Big spread of data on some features. | Features are skewed\n",
    "  * Needs to normalised\n",
    "* Some features have high correlation\n",
    "  * Feature selection required\n",
    "* Some features have high number of outliers\n",
    "  * If accuracy is hampered, we may try using outlier analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns in df_train\n",
    "df_train_cleaned=df_train.drop(['Office','index'], axis=1)\n",
    "\n",
    "# Drop unnecessary columns in df_unseen (aligning with df_train_cleaned)\n",
    "df_unseen_cleaned = df_unseen.drop(['Office', 'index', 'class'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and labels (y)\n",
    "X = df_train_cleaned.drop(columns=['class'])  # Everything except the class column\n",
    "y = df_train_cleaned['class']  # Only the class column\n",
    "\n",
    "print(\"Features and labels separated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split | 80-20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Applying one-hot encoding to the string columns on the training set\n",
    "X_train_encoded = pd.get_dummies(X_train, columns=['Music', 'Storage', 'Guitar'])\n",
    "\n",
    "# Aligning the test set with the training set encoding\n",
    "X_test_encoded = pd.get_dummies(X_test, columns=['Music', 'Storage', 'Guitar'])\n",
    "\n",
    "# Ensuring the test set has the same columns as the training set (fill missing columns with 0s)\n",
    "# Columns should be the same, but this step is just for precaution\n",
    "X_test_encoded = X_test_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)\n",
    "\n",
    "# Applying one-hot encoding to the unseen data\n",
    "df_unseen_encoded = pd.get_dummies(df_unseen_cleaned, columns=['Music', 'Storage', 'Guitar'])\n",
    "\n",
    "# Ensuring unseen data has the same columns as the training set\n",
    "df_unseen_encoded = df_unseen_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation of missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train_imputed = imputer.fit_transform(X_train_encoded)  # Fit on training data\n",
    "X_test_imputed = imputer.transform(X_test_encoded)  # Transform test data based on training data\n",
    "\n",
    "df_unseen_imputed = imputer.transform(df_unseen_encoded)  # Transform unseen data based on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking class distribution in the training set\n",
    "class_counts = np.unique(y_train, return_counts=True)\n",
    "class_distribution = dict(zip(class_counts[0], class_counts[1]))\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking class distribution in the test set\n",
    "class_counts = np.unique(y_test, return_counts=True)\n",
    "class_distribution = dict(zip(class_counts[0], class_counts[1]))\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle class imbalance on train set | Oversampling with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying SMOTE to handle class imbalance (only to the training set)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_imputed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking class distribution in the train set | After SMOTE\n",
    "class_counts = np.unique(y_train_resampled, return_counts=True)\n",
    "class_distribution = dict(zip(class_counts[0], class_counts[1]))\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking class distribution in the test set\n",
    "# To make sure it is still the same\n",
    "class_counts = np.unique(y_test, return_counts=True)\n",
    "class_distribution = dict(zip(class_counts[0], class_counts[1]))\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalizing the training data and applying the same transformation to the test data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)  # Fit and transform training data\n",
    "X_test_scaled = scaler.transform(X_test_imputed)  # Applying the same scaling from the training set\n",
    "\n",
    "# Normalizing the unseen data using the same scaler\n",
    "df_unseen_scaled = scaler.transform(df_unseen_imputed)  # Applying the same scaling from the training set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RandomForestClassifier to compute feature importances\n",
    "rf_model_importance = RandomForestClassifier(random_state=42)\n",
    "rf_model_importance.fit(X_train_scaled, y_train_resampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Feature Importance Ranking and Plotting with Original Names ---- #\n",
    "def plot_feature_importances(feature_importances, feature_names, num_features=None):\n",
    "\n",
    "    if num_features is None:\n",
    "        num_features = len(feature_importances)\n",
    "    \n",
    "    sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "    top_indices = sorted_indices[:num_features]\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.title(f\"Top {num_features} Feature Importances\")\n",
    "    plt.barh(range(num_features), feature_importances[top_indices], align='center')\n",
    "    plt.yticks(range(num_features), [feature_names[i] for i in top_indices])\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel('Importance')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting feature importances from the RF Model\n",
    "feature_importances = rf_model_importance.feature_importances_\n",
    "\n",
    "# Using the original feature names (after one-hot encoding)\n",
    "feature_names = X_train_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the feature importance for all features with original names\n",
    "plot_feature_importances(feature_importances, feature_names, num_features=len(feature_importances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects the top N features based on feature importances and prints the selected features.\n",
    "\n",
    "def select_top_n_features(X_train, X_test, feature_importances, feature_names, n):\n",
    "    \n",
    "    # Getting the top N feature indices\n",
    "    sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "    top_n_indices = sorted_indices[:n]\n",
    "    \n",
    "    # Printing the selected feature names\n",
    "    selected_features = feature_names[top_n_indices]\n",
    "    print(f\"Top {n} selected features:\")\n",
    "    for feature in selected_features:\n",
    "        print(feature)\n",
    "    \n",
    "    # Select only the top N features for training and test sets\n",
    "    X_train_top_n = X_train[:, top_n_indices]\n",
    "    X_test_top_n = X_test[:, top_n_indices]\n",
    "    \n",
    "    return X_train_top_n, X_test_top_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a KNN classifier to check how does the classifier behave with increasing features\n",
    "# This will help us to identify how many should we use\n",
    "\n",
    "# Define a range of feature counts to test. We will use all features\n",
    "n_features_range = range(1, 44)\n",
    "\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "# Iterate over the range of feature counts\n",
    "for n_features_to_select in n_features_range:\n",
    "    # Select top N features for both train and test sets\n",
    "    X_train_top_n, X_test_top_n = select_top_n_features(X_train_scaled, X_test_scaled, feature_importances, feature_names, n_features_to_select)\n",
    "\n",
    "    # Initialize the KNN model\n",
    "    knn = KNeighborsClassifier()\n",
    "\n",
    "    # Define the hyperparameter grid\n",
    "    param_grid_knn = {\n",
    "        'n_neighbors': range(10, 51, 5),\n",
    "        'weights': ['uniform'],\n",
    "        'metric': ['euclidean', 'manhattan']\n",
    "    }\n",
    "\n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search_knn = GridSearchCV(knn, param_grid_knn, cv=5, n_jobs=-1, verbose=1, return_train_score=True)\n",
    "    grid_search_knn.fit(X_train_top_n, y_train_resampled)\n",
    "\n",
    "    # Get the best KNN model from the grid search\n",
    "    best_knn_feature_plotting = grid_search_knn.best_estimator_\n",
    "\n",
    "    # Evaluate the model on the training and test sets\n",
    "    train_accuracy_knn = best_knn_feature_plotting.score(X_train_top_n, y_train_resampled)\n",
    "    test_accuracy_knn = best_knn_feature_plotting.score(X_test_top_n, y_test)\n",
    "\n",
    "    # Store the accuracies\n",
    "    train_accuracies.append(train_accuracy_knn)\n",
    "    test_accuracies.append(test_accuracy_knn)\n",
    "\n",
    "# Plot the accuracy curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_features_range, train_accuracies, label='Training Accuracy', marker='o', linestyle='-', color='blue')\n",
    "plt.plot(n_features_range, test_accuracies, label='Test Accuracy', marker='o', linestyle='-', color='green')\n",
    "\n",
    "plt.title('KNN Accuracy vs. Number of Features')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(n_features_range)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that when we select the top 12 features, we get the maximum test accuracy with minimal distance from the train accuracy. There is a big jump in test accuracy from 10 to 11. However, there is also a small jump from 11 to 12, while the train accuracy only increases slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 12 features based on importance and print the selected features\n",
    "n_features_to_select = 12\n",
    "X_train_top_n, X_test_top_n = select_top_n_features(X_train_scaled, X_test_scaled, feature_importances, feature_names, n_features_to_select)\n",
    "\n",
    "# Now X_train_top_n and X_test_top_n contain only the top 12 features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training and comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the KNN parameter grid\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': range(10, 51, 1),\n",
    "    'weights': ['uniform'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "# Create a KNN model\n",
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "# Define Stratified K-Fold cross-validator\n",
    "stratified_kfold = StratifiedKFold(n_splits=10)\n",
    "\n",
    "# Perform grid search with SKF\n",
    "grid_search_knn = GridSearchCV(knn_model, param_grid_knn, cv=stratified_kfold, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_knn.fit(X_train_top_n, y_train_resampled)\n",
    "\n",
    "# Get the best model parameters\n",
    "best_knn_model = grid_search_knn.best_estimator_\n",
    "\n",
    "# Predict on training and test data\n",
    "y_train_pred_knn = best_knn_model.predict(X_train_top_n)\n",
    "y_test_pred_knn = best_knn_model.predict(X_test_top_n)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"KNN Training Accuracy:\", accuracy_score(y_train_resampled, y_train_pred_knn))\n",
    "print(\"KNN Test Accuracy:\", accuracy_score(y_test, y_test_pred_knn))\n",
    "print(\"\\nKNN Classification Report on Test Data:\\n\", classification_report(y_test, y_test_pred_knn))\n",
    "print(\"\\nKNN Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred_knn))\n",
    "\n",
    "# Output the chosen hyperparameters\n",
    "print(\"Best hyperparameters for KNN:\", grid_search_knn.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Naive Bayes model\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "# Train the Naive Bayes model\n",
    "nb_model.fit(X_train_top_n, y_train_resampled)\n",
    "\n",
    "# Predict on training and test data\n",
    "y_train_pred_nb = nb_model.predict(X_train_top_n)\n",
    "y_test_pred_nb = nb_model.predict(X_test_top_n)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Naive Bayes Training Accuracy:\", accuracy_score(y_train_resampled, y_train_pred_nb))\n",
    "print(\"Naive Bayes Test Accuracy:\", accuracy_score(y_test, y_test_pred_nb))\n",
    "print(\"\\nNaive Bayes Classification Report on Test Data:\\n\", classification_report(y_test, y_test_pred_nb))\n",
    "print(\"\\nNaive Bayes Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred_nb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Decision Tree parameter grid\n",
    "param_grid_dt = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [10, 12, 14],\n",
    "    'min_samples_split': [10, 20],\n",
    "    'min_samples_leaf': [5, 6],\n",
    "    'class_weight': [None, {0: 1.2, 1: 1, 2: 1}]\n",
    "}\n",
    "\n",
    "# Create a Decision Tree model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "stratified_kfold = StratifiedKFold(n_splits=10)\n",
    "\n",
    "# Perform grid search with SKF\n",
    "grid_search_dt = GridSearchCV(dt_model, param_grid_dt, cv=stratified_kfold, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_dt.fit(X_train_top_n, y_train_resampled)\n",
    "\n",
    "# Get the best model parameters\n",
    "best_dt_model = grid_search_dt.best_estimator_\n",
    "\n",
    "# Predict on training and test data\n",
    "y_train_pred_dt = best_dt_model.predict(X_train_top_n)\n",
    "y_test_pred_dt = best_dt_model.predict(X_test_top_n)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Decision Tree Training Accuracy:\", accuracy_score(y_train_resampled, y_train_pred_dt))\n",
    "print(\"Decision Tree Test Accuracy:\", accuracy_score(y_test, y_test_pred_dt))\n",
    "print(\"\\nDecision Tree Classification Report on Test Data:\\n\", classification_report(y_test, y_test_pred_dt))\n",
    "print(\"\\nDecision Tree Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred_dt))\n",
    "\n",
    "# Output the chosen hyperparameters\n",
    "print(\"Best hyperparameters for Decision Tree:\", grid_search_dt.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Random Forest parameter grid\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [700, 800],\n",
    "    'criterion': ['gini'],\n",
    "    'max_depth': [13, 14],\n",
    "    'min_samples_split': [20],\n",
    "    'min_samples_leaf': [5, 6],\n",
    "    'class_weight': [{0: 1.2, 1: 1, 2: 1}]\n",
    "}\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "stratified_kfold = StratifiedKFold(n_splits=10)\n",
    "\n",
    "# Perform grid search with SKF\n",
    "grid_search_rf = GridSearchCV(rf_model, param_grid_rf, cv=stratified_kfold, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_rf.fit(X_train_top_n, y_train_resampled)\n",
    "\n",
    "# Get the best model parameters\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "\n",
    "# Predict on training and test data\n",
    "y_train_pred_rf = best_rf_model.predict(X_train_top_n)\n",
    "y_test_pred_rf = best_rf_model.predict(X_test_top_n)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Random Forest Training Accuracy:\", accuracy_score(y_train_resampled, y_train_pred_rf))\n",
    "print(\"Random Forest Test Accuracy:\", accuracy_score(y_test, y_test_pred_rf))\n",
    "print(\"\\nRandom Forest Classification Report on Test Data:\\n\", classification_report(y_test, y_test_pred_rf))\n",
    "print(\"\\nRandom Forest Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred_rf))\n",
    "\n",
    "# Output the chosen hyperparameters\n",
    "print(\"Best hyperparameters for Random Forest:\", grid_search_rf.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. MLP Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP parameter grid\n",
    "param_grid_mlp = {\n",
    "    'hidden_layer_sizes': [(50,), (30, 30), (30, 15), (50, 25)],\n",
    "    'activation': ['tanh', 'relu', 'logistic'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    'momentum': [0.9, 0.95],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "    'learning_rate_init': [0.001, 0.0005, 0.0001]\n",
    "}\n",
    "\n",
    "# Create an MLP model with early stopping\n",
    "mlp_model = MLPClassifier(max_iter=1500, random_state=42, early_stopping=True)\n",
    "\n",
    "stratified_kfold = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Perform grid search with SKF\n",
    "grid_search_mlp = GridSearchCV(mlp_model, param_grid_mlp, cv=stratified_kfold, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_mlp.fit(X_train_top_n, y_train_resampled)\n",
    "\n",
    "# Get the best model parameters\n",
    "best_mlp_model = grid_search_mlp.best_estimator_\n",
    "\n",
    "# Predict on training and test data\n",
    "y_train_pred_mlp = best_mlp_model.predict(X_train_top_n)\n",
    "y_test_pred_mlp = best_mlp_model.predict(X_test_top_n)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"MLP Training Accuracy:\", accuracy_score(y_train_resampled, y_train_pred_mlp))\n",
    "print(\"MLP Test Accuracy:\", accuracy_score(y_test, y_test_pred_mlp))\n",
    "print(\"\\nMLP Classification Report on Test Data:\\n\", classification_report(y_test, y_test_pred_mlp))\n",
    "print(\"\\nMLP Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred_mlp))\n",
    "\n",
    "# Output the chosen hyperparameters\n",
    "print(\"Best hyperparameters for MLP:\", grid_search_mlp.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "# Defining the SVM parameter grid\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10, 100],  \n",
    "    'kernel': ['linear', 'rbf'],  \n",
    "    'gamma': ['scale', 'auto']  \n",
    "}\n",
    "\n",
    "# Creating an SVM model\n",
    "svm_model = SVC(probability=True, random_state=42)\n",
    "\n",
    "# Defining Stratified K-Fold cross-validator\n",
    "stratified_kfold = StratifiedKFold(n_splits=10)\n",
    "\n",
    "# Performing grid search with SKF\n",
    "grid_search_svm = GridSearchCV(svm_model, param_grid_svm, cv=stratified_kfold, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_svm.fit(X_train_top_n, y_train_resampled)\n",
    "\n",
    "# Get the best model parameters\n",
    "best_svm_model = grid_search_svm.best_estimator_\n",
    "\n",
    "# Predict on training and test data\n",
    "y_train_pred_svm = best_svm_model.predict(X_train_top_n)\n",
    "y_test_pred_svm = best_svm_model.predict(X_test_top_n)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"SVM Training Accuracy:\", accuracy_score(y_train_resampled, y_train_pred_svm))\n",
    "print(\"SVM Test Accuracy:\", accuracy_score(y_test, y_test_pred_svm))\n",
    "print(\"\\nSVM Classification Report on Test Data:\\n\", classification_report(y_test, y_test_pred_svm))\n",
    "print(\"\\nSVM Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred_svm))\n",
    "\n",
    "# Output the chosen hyperparameters\n",
    "print(\"Best hyperparameters for SVM:\", grid_search_svm.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['KNN', 'Naive Bayes', 'Decision Tree', 'Random Forest', 'MLP', 'SVM']\n",
    "train_accuracies = [\n",
    "    accuracy_score(y_train_resampled, y_train_pred_knn),\n",
    "    accuracy_score(y_train_resampled, y_train_pred_nb),\n",
    "    accuracy_score(y_train_resampled, y_train_pred_dt),\n",
    "    accuracy_score(y_train_resampled, y_train_pred_rf),\n",
    "    accuracy_score(y_train_resampled, y_train_pred_mlp),\n",
    "    accuracy_score(y_train_resampled, y_train_pred_svm)  \n",
    "]\n",
    "test_accuracies = [\n",
    "    accuracy_score(y_test, y_test_pred_knn),\n",
    "    accuracy_score(y_test, y_test_pred_nb),\n",
    "    accuracy_score(y_test, y_test_pred_dt),\n",
    "    accuracy_score(y_test, y_test_pred_rf),\n",
    "    accuracy_score(y_test, y_test_pred_mlp),\n",
    "    accuracy_score(y_test, y_test_pred_svm)  \n",
    "]\n",
    "\n",
    "# Plot train vs test accuracies for each model\n",
    "x = range(len(models))\n",
    "\n",
    "plt.figure(figsize=(10, 6))  \n",
    "plt.bar(x, train_accuracies, width=0.4, label='Train Accuracy', align='center')\n",
    "plt.bar(x, test_accuracies, width=0.4, label='Test Accuracy', align='edge')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(x, models, rotation=30, ha='right')  \n",
    "plt.legend()\n",
    "plt.title('Train vs Test Accuracy for Each Model')\n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['KNN', 'Naive Bayes', 'Decision Tree', 'Random Forest', 'MLP', 'SVM']\n",
    "y_preds = [y_test_pred_knn, y_test_pred_nb, y_test_pred_dt, y_test_pred_rf, y_test_pred_mlp, y_test_pred_svm]\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, y_pred in enumerate(y_preds):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i], cbar=False)\n",
    "    axes[i].set_title(models[i])\n",
    "    axes[i].set_xlabel('Predicted Label')\n",
    "    axes[i].set_ylabel('True Label')\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['KNN', 'Naive Bayes', 'Decision Tree', 'Random Forest', 'MLP', 'SVM']\n",
    "\n",
    "knn_report = classification_report(y_test, y_test_pred_knn, output_dict=True)\n",
    "nb_report = classification_report(y_test, y_test_pred_nb, output_dict=True)\n",
    "dt_report = classification_report(y_test, y_test_pred_dt, output_dict=True)\n",
    "rf_report = classification_report(y_test, y_test_pred_rf, output_dict=True)\n",
    "mlp_report = classification_report(y_test, y_test_pred_mlp, output_dict=True)\n",
    "svm_report = classification_report(y_test, y_test_pred_svm, output_dict=True)\n",
    "\n",
    "f1_scores = [\n",
    "    knn_report['macro avg']['f1-score'],\n",
    "    nb_report['macro avg']['f1-score'],\n",
    "    dt_report['macro avg']['f1-score'],\n",
    "    rf_report['macro avg']['f1-score'],\n",
    "    mlp_report['macro avg']['f1-score'],\n",
    "    svm_report['macro avg']['f1-score']\n",
    "]\n",
    "\n",
    "precision_scores = [\n",
    "    knn_report['macro avg']['precision'],\n",
    "    nb_report['macro avg']['precision'],\n",
    "    dt_report['macro avg']['precision'],\n",
    "    rf_report['macro avg']['precision'],\n",
    "    mlp_report['macro avg']['precision'],\n",
    "    svm_report['macro avg']['precision']\n",
    "]\n",
    "\n",
    "recall_scores = [\n",
    "    knn_report['macro avg']['recall'],\n",
    "    nb_report['macro avg']['recall'],\n",
    "    dt_report['macro avg']['recall'],\n",
    "    rf_report['macro avg']['recall'],\n",
    "    mlp_report['macro avg']['recall'],\n",
    "    svm_report['macro avg']['recall']\n",
    "]\n",
    "\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.25\n",
    "\n",
    "plt.figure(figsize=(10, 6))  \n",
    "plt.bar(x - width, f1_scores, width, label='F1 Score')\n",
    "plt.bar(x, precision_scores, width, label='Precision')\n",
    "plt.bar(x + width, recall_scores, width, label='Recall')\n",
    "\n",
    "plt.xticks(x, models, rotation=30, ha='right') \n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Scores')\n",
    "plt.legend()\n",
    "plt.title('F1 Score, Precision, and Recall for Each Model')\n",
    "plt.tight_layout()  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_binarized = label_binarize(y_test, classes=[0, 1, 2])  \n",
    "\n",
    "y_score_knn = best_knn_model.predict_proba(X_test_top_n)\n",
    "y_score_nb = nb_model.predict_proba(X_test_top_n)\n",
    "y_score_dt = best_dt_model.predict_proba(X_test_top_n)\n",
    "y_score_rf = best_rf_model.predict_proba(X_test_top_n)\n",
    "y_score_mlp = best_mlp_model.predict_proba(X_test_top_n)\n",
    "y_score_svm = best_svm_model.predict_proba(X_test_top_n)\n",
    "\n",
    "y_scores = [y_score_knn, y_score_nb, y_score_dt, y_score_rf, y_score_mlp, y_score_svm]\n",
    "models = ['KNN', 'Naive Bayes', 'Decision Tree', 'Random Forest', 'MLP', 'SVM']\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, y_score in enumerate(y_scores):\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "    \n",
    "    for j in range(len(y_test_binarized[0])):  \n",
    "        fpr[j], tpr[j], _ = roc_curve(y_test_binarized[:, j], y_score[:, j])\n",
    "        roc_auc[j] = auc(fpr[j], tpr[j])\n",
    "\n",
    "    for j in range(len(y_test_binarized[0])):\n",
    "        axes[i].plot(fpr[j], tpr[j], label=f'Class {j} (AUC = {roc_auc[j]:.2f})')\n",
    "\n",
    "    axes[i].plot([0, 1], [0, 1], 'k--')  \n",
    "    axes[i].set_xlim([0.0, 1.0])\n",
    "    axes[i].set_ylim([0.0, 1.05])\n",
    "    axes[i].set_xlabel('False Positive Rate')\n",
    "    axes[i].set_ylabel('True Positive Rate')\n",
    "    axes[i].set_title(f'ROC Curve for {models[i]}')\n",
    "    axes[i].legend(loc=\"lower right\")\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best two models are: KNN and Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on Unseen Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the top 12 features from df_unseen_scaled\n",
    "top_n_indices = np.argsort(feature_importances)[::-1][:n_features_to_select]\n",
    "df_unseen_top_n = df_unseen_scaled[:, top_n_indices]\n",
    "\n",
    "# Get the names of the top 12 features\n",
    "top_n_feature_names = feature_names[top_n_indices]\n",
    "\n",
    "# Print the top 12 feature names\n",
    "print(\"Top 12 features selected from df_unseen:\")\n",
    "for feature in top_n_feature_names:\n",
    "    print(feature)\n",
    "\n",
    "# Create a DataFrame for df_unseen_top_n with the top 12 feature names\n",
    "df_unseen_top_n_df = pd.DataFrame(df_unseen_top_n, columns=top_n_feature_names)\n",
    "\n",
    "# Display the first few rows of the DataFrame to inspect the values\n",
    "print(\"Top 12 features in df_unseen (first 5 rows):\")\n",
    "print(df_unseen_top_n_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class labels for df_unseen using the finalized KNN and MLP models \n",
    "\n",
    "# Predict using the KNN model with the top 12 features\n",
    "y_unseen_pred_knn = best_knn_model.predict(df_unseen_top_n)\n",
    "\n",
    "# Predict using the Neural Network model with the top 12 features\n",
    "y_unseen_pred_mlp = best_mlp_model.predict(df_unseen_top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output DataFrame with index, KNN predictions, and Neural Network predictions\n",
    "output_df = pd.DataFrame({\n",
    "    'index': df_unseen['index'],  \n",
    "    'KNN': y_unseen_pred_knn,\n",
    "    'NeuralNetwork': y_unseen_pred_mlp\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to Sqlite file\n",
    "output_cnx = db.connect('Answers.sqlite')\n",
    "\n",
    "output_df.to_sql('predictions', output_cnx, if_exists='replace', index=False)\n",
    "\n",
    "output_cnx.close()\n",
    "\n",
    "print(\"Predictions for KNN and Neural Network saved to 'Answers.sqlite' successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Taking a look at the output_df\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analysing model disagreement\n",
    "\n",
    "# Filter rows where KNN is not equal to NeuralNetwork\n",
    "difference_df = output_df[output_df['KNN'] != output_df['NeuralNetwork']]\n",
    "\n",
    "# Count of differences\n",
    "count_differences = difference_df.shape[0]\n",
    "\n",
    "# Total number of rows\n",
    "total_count = output_df.shape[0]\n",
    "\n",
    "# Percentage of differences\n",
    "percentage_difference = (count_differences / total_count) * 100 if total_count > 0 else 0\n",
    "\n",
    "# Display the results\n",
    "print(\"Count of differences:\", count_differences)\n",
    "print(\"Percentage of differences: {:.2f}%\".format(percentage_difference))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, there is a possibility that the classifiers are giving a wrong prediction 10.40% of the time, where as they are correct 89.6%\n",
    "\n",
    "KNN Accuracy from grid search: 0.89 == 89%\n",
    "Neural Network Accuracy from grid search: 0.904 == 90.4%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
